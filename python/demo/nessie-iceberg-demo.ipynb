{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nessie Demo\n",
    "===========\n",
    "This demo showcases how to use Nessie python API along with Spark3 from Iceberg\n",
    "\n",
    "Initialize Pyspark + Nessie environment\n",
    "----------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import findspark\n",
    "from pyspark.sql import *\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from py4j.java_gateway import java_import\n",
    "findspark.init()\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.set(\"spark.jars\", \"/path/to/iceberg-spark3-runtime-apache-iceberg-0.11.0.jar \")\n",
    "conf.set(\"spark.sql.execution.pyarrow.enabled\", \"true\")\n",
    "conf.set(\"spark.sql.catalog.nessie.warehouse\", 'file://' + os.getcwd() + '/spark_warehouse')\n",
    "conf.set(\"spark.sql.catalog.nessie.url\", \"http://localhost:19120/api/v1\")\n",
    "conf.set(\"spark.sql.catalog.nessie.ref\", \"main\")\n",
    "conf.set(\"spark.sql.catalog.nessie.catalog-impl\", \"org.apache.iceberg.nessie.NessieCatalog\")\n",
    "conf.set(\"spark.sql.catalog.nessie.auth_type\", \"NONE\")\n",
    "conf.set(\"spark.sql.catalog.nessie.cache-enabled\", \"false\")\n",
    "conf.set(\"spark.sql.catalog.nessie\", \"org.apache.iceberg.spark.SparkCatalog\")\n",
    "conf.set(\"spark.sql.catalog.spark_catalog\", \"org.apache.iceberg.spark.SparkSessionCatalog\")\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "jvm = sc._gateway.jvm\n",
    "\n",
    "java_import(jvm, \"org.apache.iceberg.nessie.NessieCatalog\")\n",
    "java_import(jvm, \"org.apache.iceberg.catalog.TableIdentifier\")\n",
    "java_import(jvm, \"org.apache.iceberg.Schema\")\n",
    "java_import(jvm, \"org.apache.iceberg.types.Types\")\n",
    "java_import(jvm, \"org.apache.iceberg.PartitionSpec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Extra Spark Contexts\n",
    "------------------------------\n",
    "\n",
    "Because of the way Spark Sessions cache Catalogs we can only have 1 Nessie Catalog per Spark Session. Below we create a few Spark Sessions to deal with the different Nessie branches we will use in the course of the Demo. Typically in a real life scenario we would create one Nessie catalog per Spark insance and each of the below sections would be run by different users on different branches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# session for dev branch\n",
    "spark_dev = spark.newSession()\n",
    "spark_dev.conf.set(\"spark.sql.catalog.nessie.ref\", \"dev\" )\n",
    "\n",
    "# session for ETL branch\n",
    "spark_etl = spark.newSession()\n",
    "spark_etl.conf.set(\"spark.sql.catalog.nessie.ref\", \"etl\" )\n",
    "\n",
    "# session for experiment branch\n",
    "spark_experiment = spark.newSession()\n",
    "spark_experiment.conf.set(\"spark.sql.catalog.nessie.ref\", \"experiment\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up nessie branches\n",
    "----------------------------\n",
    "\n",
    "- Branch `main` already exists\n",
    "- Create branch `dev`\n",
    "- List all branches (pipe JSON result into jq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nessie branch dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nessie --verbose branch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create tables under dev branch\n",
    "-------------------------------------\n",
    "\n",
    "Creating two tables under the `dev` branch:\n",
    "- region\n",
    "- nation\n",
    "\n",
    "It is not yet possible to create table using pyspark and iceberg, so Java code\n",
    "is used instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog = jvm.NessieCatalog()\n",
    "catalog.setConf(sc._jsc.hadoopConfiguration())\n",
    "catalog.initialize(\"nessie\", {'ref': 'dev', 'url': 'http://localhost:19120/api/v1', \"warehouse\": 'file://' + os.getcwd() + '/spark_warehouse'})\n",
    "\n",
    "# Creating region table\n",
    "region_name = jvm.TableIdentifier.parse(\"testing.region\")\n",
    "region_schema = jvm.Schema([\n",
    "    jvm.Types.NestedField.optional(1, \"R_REGIONKEY\", jvm.Types.LongType.get()),\n",
    "    jvm.Types.NestedField.optional(2, \"R_NAME\", jvm.Types.StringType.get()),\n",
    "    jvm.Types.NestedField.optional(3, \"R_COMMENT\", jvm.Types.StringType.get()),\n",
    "])\n",
    "region_spec = jvm.PartitionSpec.unpartitioned()\n",
    "\n",
    "region_table = catalog.createTable(region_name, region_schema, region_spec)\n",
    "region_df = spark_dev.read.load(\"data/region.parquet\")\n",
    "region_df.write.format(\"iceberg\").mode(\"overwrite\").save(\"nessie.testing.region\")\n",
    "\n",
    "# Creating nation table\n",
    "nation_name = jvm.TableIdentifier.parse(\"testing.nation\")\n",
    "nation_schema = jvm.Schema([\n",
    "    jvm.Types.NestedField.optional(1, \"N_NATIONKEY\", jvm.Types.LongType.get()),\n",
    "    jvm.Types.NestedField.optional(2, \"N_NAME\", jvm.Types.StringType.get()),\n",
    "    jvm.Types.NestedField.optional(3, \"N_REGIONKEY\", jvm.Types.LongType.get()),\n",
    "    jvm.Types.NestedField.optional(4, \"N_COMMENT\", jvm.Types.StringType.get()),\n",
    "])\n",
    "nation_spec = jvm.PartitionSpec.builderFor(nation_schema).truncate(\"N_NAME\", 2).build()\n",
    "nation_table = catalog.createTable(nation_name, nation_schema, nation_spec)\n",
    "\n",
    "nation_df = spark_dev.read.load(\"data/nation.parquet\")\n",
    "nation_df.write.format(\"iceberg\").mode(\"overwrite\").save(\"nessie.testing.nation\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check generated tables\n",
    "----------------------------\n",
    "\n",
    "Check tables generated under the dev branch (and that the main branch does not\n",
    "have any tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nessie contents --list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nessie contents --list --ref dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the `dev` and `main` branches point to different commits now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nessie --verbose branch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dev promotion\n",
    "-------------\n",
    "\n",
    "Promote dev branch promotion to main.\n",
    "\n",
    "* main now has the same tables as dev\n",
    "* main and dev point to the same commit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nessie merge dev -b main --force"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nessie contents --list --ref etl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nessie --verbose branch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create `etl` branch\n",
    "----------------------\n",
    "\n",
    "- Create a branch `etl` out of `main`\n",
    "- add data to nation\n",
    "- alter the schema of region\n",
    "- create table city\n",
    "- query the tables in `etl`\n",
    "- query the tables in `main`\n",
    "- promote `etl` branch to `main`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nessie branch etl main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nation = Row(\"N_NATIONKEY\", \"N_NAME\", \"N_REGIONKEY\", \"N_COMMENT\")\n",
    "new_nations = spark_etl.createDataFrame([\n",
    "    Nation(25, \"SYLDAVIA\", 3, \"King Ottokar's Sceptre\"),\n",
    "    Nation(26, \"SAN THEODOROS\", 1, \"The Picaros\")])\n",
    "new_nations.write.format(\"iceberg\").mode(\"append\").save(\"nessie.testing.nation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing the default branch\n",
    "etl_catalog = jvm.NessieCatalog()\n",
    "etl_catalog.setConf(sc._jsc.hadoopConfiguration())\n",
    "etl_catalog.initialize(\"etl\", {'ref': 'etl', 'url': 'http://localhost:19120/api/v1', \"warehouse\": 'file://' + os.getcwd() + '/spark_warehouse'})\n",
    "etl_catalog.loadTable(region_name).updateSchema().addColumn('R_ABBREV', jvm.Types.StringType.get()).commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating city table\n",
    "spark_etl.sql(\"CREATE TABLE nessie.testing.city (C_CITYKEY BIGINT, C_NAME STRING, N_NATIONKEY BIGINT, C_COMMNT STRING) USING iceberg PARTITIONED BY (N_NATIONKEY)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pynessie import init\n",
    "nessie = init()\n",
    "nessie.list_keys('main').entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[i.name for i in nessie.list_keys('etl').entries]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{i.name:i.hash_ for i in nessie.list_references()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nessie.merge('main', 'etl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{i.name:i.hash_ for i in nessie.list_references()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create `experiment` branch\n",
    "--------------------------------\n",
    "\n",
    "- create `experiment` branch from `main`\n",
    "- drop `nation` table\n",
    "- add data to `region` table\n",
    "- compare `experiment` and `main` tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nessie branch experiment main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing the default branch\n",
    "catalog = jvm.NessieCatalog()\n",
    "catalog.setConf(sc._jsc.hadoopConfiguration())\n",
    "catalog.initialize(\"experiment\", {'ref': 'experiment', 'url': 'http://localhost:19120/api/v1', \"warehouse\": 'file://' + os.getcwd() + '/spark_warehouse'})\n",
    "\n",
    "catalog.dropTable(jvm.TableIdentifier.parse(\"testing.nation\"), False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_experiment.sql('INSERT INTO TABLE nessie.testing.region VALUES (5, \"AUSTRALIA\", \"Let\\'s hop there\", \"AUS\")')\n",
    "spark_experiment.sql('INSERT INTO TABLE nessie.testing.region VALUES (6, \"ANTARTICA\", \"It\\'s cold\", \"ANT\")')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nessie contents --list --ref experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets take a look at the contents of the region table on the experiment branch.\n",
    "Notice the use of the `nessie` catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select * from nessie.testing.`region@experiment`\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and compare to the contents of the region table on the main branch. Notice the\n",
    "use of `@main` to view data on the main branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select * from nessie.testing.region\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
